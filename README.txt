This repository contains the import kaggle notebooks that can be used for reference in the future
NLP
1. utilizing-transformer-representations-efficiently
	This notebook shows that we can not only use the last hidden layer of attention based model for the subsequent FC/GRU/CRF layers